{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-18T16:53:24.521808Z",
     "iopub.status.busy": "2025-04-18T16:53:24.521178Z",
     "iopub.status.idle": "2025-04-18T16:53:24.531127Z",
     "shell.execute_reply": "2025-04-18T16:53:24.530585Z",
     "shell.execute_reply.started": "2025-04-18T16:53:24.521786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataLoaderHelper:\n",
    "    def __init__(self, train_data_dir,test_data_dir, input_size, batch_size, augmentation):\n",
    "        self.data_dir = train_data_dir\n",
    "        self.test_dir = test_data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation = augmentation\n",
    "        self.input_size = input_size  # tuple like (400, 400)\n",
    "\n",
    "        self.transform = self.get_transform()\n",
    "        self.train_data, self.val_data = self.load_train_val_data()\n",
    "        self.test_data = self.load_test_data()\n",
    "\n",
    "    def get_transform(self):\n",
    "        if self.augmentation:\n",
    "            #Rotating the image.\n",
    "            transforms_list = [\n",
    "                transforms.RandomResizedCrop(self.input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        else:\n",
    "            transforms_list = [\n",
    "                transforms.Resize(self.input_size),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "\n",
    "        #Normalize the input for better performance\n",
    "        transforms_list.append(\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        )\n",
    "        return transforms.Compose(transforms_list)\n",
    "\n",
    "    def load_train_val_data(self):\n",
    "        full_dataset = datasets.ImageFolder(root=self.data_dir, transform=self.transform)\n",
    "        total_size = len(full_dataset)\n",
    "        indices = list(range(total_size))\n",
    "\n",
    "        #Train_test_split does not allow tensor data.. (So need to split based on the indices)\n",
    "        train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "        # print(f\"Total: {total_size} | Train: {len(train_idx)} | Val: {len(val_idx)}\")\n",
    "        return Subset(full_dataset, train_idx), Subset(full_dataset, val_idx)\n",
    "\n",
    "    def load_test_data(self):\n",
    "        return datasets.ImageFolder(root=self.test_dir, transform=self.transform)\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        #Addded pin_memory and workers to increase the loading speed\n",
    "        train_loader = DataLoader(self.train_data, batch_size=self.batch_size,\n",
    "                                  shuffle=True, num_workers=2, pin_memory=True)\n",
    "        val_loader = DataLoader(self.val_data, batch_size=self.batch_size,\n",
    "                                shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        test_loader = DataLoader(self.test_data, batch_size=self.batch_size,\n",
    "                                     shuffle=False, num_workers=2, pin_memory=True)\n",
    "      \n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T17:09:16.282715Z",
     "iopub.status.busy": "2025-04-18T17:09:16.282137Z",
     "iopub.status.idle": "2025-04-18T17:09:16.287371Z",
     "shell.execute_reply": "2025-04-18T17:09:16.286641Z",
     "shell.execute_reply.started": "2025-04-18T17:09:16.282692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "class FinetuneCNN():\n",
    "    def __init__(self,pt=True):\n",
    "      self.fine_tune_model=models.resnet50(pretrained=pt)\n",
    "\n",
    "    def Freezelayers(self):\n",
    "        for i in self.fine_tune_model.parameters():\n",
    "            i.requires_grad=False\n",
    "\n",
    "        features=self.fine_tune_model.fc.in_features\n",
    "        self.fine_tune_model.fc= nn.Linear(features, 10)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:54:04.763217Z",
     "iopub.status.busy": "2025-04-18T16:54:04.762832Z",
     "iopub.status.idle": "2025-04-18T16:54:07.688675Z",
     "shell.execute_reply": "2025-04-18T16:54:07.688113Z",
     "shell.execute_reply.started": "2025-04-18T16:54:04.763186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader,test_loader, optimizer_name, learning_rate, num_epochs,weight_decay):\n",
    "        # Set device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # self.model=torch.nn.DataParallel(model,device_ids = [0,1]).to(self.device)\n",
    "        self.model = model.to(self.device)\n",
    "        #Note : If you are using multidevice gpu commment the above line and uncomment the previous above line \n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader=test_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weight_decay= weight_decay\n",
    "        self.learning_rate=learning_rate\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if optimizer_name.lower() == 'adam':\n",
    "            self.optimizer = optim.Adam(model.parameters(), lr=self.learning_rate,weight_decay=self.weight_decay)\n",
    "        elif optimizer_name.lower() == 'nadam':\n",
    "            self.optimizer = optim.NAdam(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        elif optimizer_name.lower() == 'rmsprop':\n",
    "            self.optimizer = optim.RMSprop(model.parameters(), lr=self.learning_rate,weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "            \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        #Initialize the training requirements that we have defined in model\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            #weight update\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate(self):\n",
    "        #Set for evalution \n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.val_loader, desc=\"Validating\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        epoch_loss = running_loss / len(self.val_loader)\n",
    "        epoch_acc = correct / total\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "            \n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc = self.validate()\n",
    "            \n",
    "            # Store history for analysis\n",
    "            self.train_loss_history.append(train_loss)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "            self.train_acc_history.append(train_acc)\n",
    "            self.val_acc_history.append(val_acc)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.4f}\")\n",
    "            #Added for removing the cache\n",
    "            torch.cuda.empty_cache() \n",
    "\n",
    "    def confusion_matrix(self,count=3,Capture_Img=True,plot=True):\n",
    "        class_names=[\"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\", \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"]\n",
    "    \n",
    "        confusion_matrix = np.zeros((len(class_names), len(class_names)), dtype=int)\n",
    "        #Set for evalution\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "            \n",
    "        captured_samples = [[] for _ in range(len(class_names))] \n",
    "  \n",
    "        class_to_idx = {name: idx for idx, name in enumerate(class_names)} \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.test_loader, desc=\"testing\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "        \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Updating Confusion Matrix\n",
    "                for true, predict in zip(labels.cpu(), predicted.cpu()):\n",
    "                  confusion_matrix[true,predict] += 1\n",
    "\n",
    "                if Capture_Img:\n",
    "                    for i in range(images.size(0)):\n",
    "                        true_label = labels[i].item()\n",
    "                        class_idx = class_to_idx[class_names[true_label]] \n",
    "                        if len(captured_samples[class_idx]) < count:\n",
    "                            img = images[i].cpu().numpy()\n",
    "                            pred_label = predicted[i].item()\n",
    "                            captured_samples[class_idx].append((img, true_label, pred_label))\n",
    "\n",
    "        Acc=correct/total\n",
    "\n",
    "        print(f\"Test Acc: {Acc*100:.4f}\")\n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.xlabel(\"Predicted Label\")\n",
    "            plt.ylabel(\"Actual Label\")   \n",
    "            plt.title(\"Confusion Matrix\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.xlabel(\"Actual Label\")\n",
    "            plt.ylabel(\"Predicted Label\")\n",
    "            plt.title(\"Confusion Matrix\")\n",
    "            Img_name=\"confusion_matrix.png\"\n",
    "            plt.savefig(Img_name)\n",
    "            plt.close()\n",
    "            wandb.log({\"confusion_matrix\": wandb.Image(Img_name)})\n",
    "\n",
    "        #Capture images if flag is set and count is not reached (Capture flag is to plot the 3*10 Images)\n",
    "        if Capture_Img and any(captured_samples):\n",
    "            rows = len(class_names)\n",
    "            cols = count\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "            \n",
    "            for row_idx, class_samples in enumerate(captured_samples):\n",
    "                for col_idx in range(cols):\n",
    "                    ax = axes[row_idx, col_idx]\n",
    "                    ax.axis('off')\n",
    "                    if col_idx < len(class_samples):\n",
    "                        img, true, pred = class_samples[col_idx]\n",
    "                        # Unnormalize and transpose\n",
    "                        img = img.transpose(1, 2, 0)\n",
    "                        img = (img * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "                        img = np.clip(img, 0, 1)\n",
    "                        ax.imshow(img)\n",
    "                        ax.set_title(f\"True: {class_names[true]}\\nPred: {class_names[pred]}\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if plot:\n",
    "                plt.show()\n",
    "            else:\n",
    "                Img_name = \"predictions.png\"\n",
    "                fig.savefig(Img_name, bbox_inches='tight', dpi=300, pad_inches=0.1)\n",
    "                plt.close(fig) \n",
    "                wandb.log({\"predictions\": wandb.Image(Img_name)})\n",
    "            \n",
    "    \n",
    "    \n",
    "        return confusion_matrix\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:55:43.429130Z",
     "iopub.status.busy": "2025-04-18T16:55:43.428215Z",
     "iopub.status.idle": "2025-04-18T16:55:47.174343Z",
     "shell.execute_reply": "2025-04-18T16:55:47.173611Z",
     "shell.execute_reply.started": "2025-04-18T16:55:43.429076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:54:39.926920Z",
     "iopub.status.busy": "2025-04-18T16:54:39.926643Z",
     "iopub.status.idle": "2025-04-18T16:54:45.967298Z",
     "shell.execute_reply": "2025-04-18T16:54:45.966675Z",
     "shell.execute_reply.started": "2025-04-18T16:54:39.926900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m042\u001b[0m (\u001b[33mcs24m042-iit-madras-foundation\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "# If you are running this file in kaggle pass   key='Your login key' i.e wandb.login(key= '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:35:59.464432Z",
     "iopub.status.busy": "2025-04-18T19:35:59.464167Z",
     "iopub.status.idle": "2025-04-18T19:46:52.079403Z",
     "shell.execute_reply": "2025-04-18T19:46:52.078642Z",
     "shell.execute_reply.started": "2025-04-18T19:35:59.464410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m042\u001b[0m (\u001b[33mcs24m042-iit-madras-foundation\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_193604-p3n8qzkh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33moptimizer nadam   batch_size 32 augmentation false weight_decay 0 learning_rate 0.0001  \u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/runs/p3n8qzkh\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "\n",
      "Epoch 1/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:52<00:00,  4.73it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:12<00:00,  4.89it/s]\n",
      "Train Loss: 1.7984 | Train Acc: 50.7563\n",
      "Val Loss: 1.3936 | Val Acc: 66.0500\n",
      "\n",
      "Epoch 2/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:47<00:00,  5.22it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:11<00:00,  5.34it/s]\n",
      "Train Loss: 1.2583 | Train Acc: 68.1710\n",
      "Val Loss: 1.1022 | Val Acc: 69.9000\n",
      "\n",
      "Epoch 3/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:48<00:00,  5.19it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:11<00:00,  5.35it/s]\n",
      "Train Loss: 1.0619 | Train Acc: 70.5713\n",
      "Val Loss: 0.9708 | Val Acc: 71.9000\n",
      "\n",
      "Epoch 4/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:48<00:00,  5.18it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:11<00:00,  5.37it/s]\n",
      "Train Loss: 0.9590 | Train Acc: 72.0590\n",
      "Val Loss: 0.9084 | Val Acc: 72.0000\n",
      "\n",
      "Epoch 5/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:47<00:00,  5.26it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:12<00:00,  5.05it/s]\n",
      "Train Loss: 0.9019 | Train Acc: 72.7591\n",
      "Val Loss: 0.8711 | Val Acc: 72.5500\n",
      "\n",
      "Epoch 6/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:47<00:00,  5.24it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:12<00:00,  5.17it/s]\n",
      "Train Loss: 0.8678 | Train Acc: 73.7467\n",
      "Val Loss: 0.8372 | Val Acc: 73.3500\n",
      "\n",
      "Epoch 7/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:47<00:00,  5.23it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:12<00:00,  5.04it/s]\n",
      "Train Loss: 0.8373 | Train Acc: 74.3093\n",
      "Val Loss: 0.8299 | Val Acc: 73.8500\n",
      "\n",
      "Epoch 8/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:47<00:00,  5.24it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:11<00:00,  5.42it/s]\n",
      "Train Loss: 0.8135 | Train Acc: 74.7843\n",
      "Val Loss: 0.8145 | Val Acc: 74.6000\n",
      "\n",
      "Epoch 9/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:48<00:00,  5.13it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:11<00:00,  5.35it/s]\n",
      "Train Loss: 0.7949 | Train Acc: 74.9844\n",
      "Val Loss: 0.7974 | Val Acc: 74.3000\n",
      "\n",
      "Epoch 10/10\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:48<00:00,  5.18it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:11<00:00,  5.42it/s]\n",
      "Train Loss: 0.7815 | Train Acc: 75.4344\n",
      "Val Loss: 0.8046 | Val Acc: 73.8000\n",
      "testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:28<00:00,  2.17it/s]\n",
      "Test Acc: 74.4000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 75.43443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.78151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 73.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.80461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33moptimizer nadam   batch_size 32 augmentation false weight_decay 0 learning_rate 0.0001  \u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2/runs/p3n8qzkh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m042-iit-madras-foundation/DA6401-Assignment-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_193604-p3n8qzkh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/input/model222/other/default/1/train.py -br \"/kaggle/input/dataset1/inaturalist_12K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T17:22:52.563597Z",
     "iopub.status.busy": "2025-04-18T17:22:52.563272Z",
     "iopub.status.idle": "2025-04-18T19:00:57.299375Z",
     "shell.execute_reply": "2025-04-18T19:00:57.298542Z",
     "shell.execute_reply.started": "2025-04-18T17:22:52.563572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "input_dim=(224,224)\n",
    "num_classes=10\n",
    "\n",
    "# Add your directory here \n",
    "train_directory='/kaggle/input/dataset1/inaturalist_12K/train'\n",
    "test_directory='/kaggle/input/dataset1/inaturalist_12K/val'\n",
    "epochs=10\n",
    "\n",
    "# Sweep configuration dictionary for wandb\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name' : 'cnn-finetunining',\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'weight_decay': {\n",
    "            'values':[0, 0.0005, 0.5]\n",
    "        },\n",
    "        'augmentation': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "       \n",
    "        'learning_rate': {\n",
    "            'values': [1e-3, 1e-4]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['nadam', 'adam']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "\n",
    "        run.name = \"optimizer {}   batch_size {} augmentation {} weight_decay {} learning_rate {}  \".format(\n",
    "            config.optimizer,\n",
    "            config.batch_size,\n",
    "            config.augmentation,\n",
    "            config.weight_decay,\n",
    "            config.learning_rate\n",
    "          )\n",
    "        # Initialize data loaders\n",
    "        data_loader = DataLoaderHelper(\n",
    "            train_directory,test_data_dir=test_directory,\n",
    "            input_size=input_dim,\n",
    "            batch_size=config.batch_size,\n",
    "            augmentation=config.augmentation\n",
    "        )\n",
    "        train_loader, val_loader,test_loader = data_loader.get_dataloaders()\n",
    "        \n",
    "        # Initialize model\n",
    "        model=FinetuneCNN()\n",
    "        model.Freezelayers()\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model.fine_tune_model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            optimizer_name=config.optimizer,\n",
    "            learning_rate=config.learning_rate,\n",
    "            num_epochs=epochs,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Log final metrics\n",
    "        for epoch in range(epochs):\n",
    "            wandb.log({\n",
    "                'train_accuracy': trainer.train_acc_history[epoch]*100,\n",
    "                'train_loss': trainer.train_loss_history[epoch],\n",
    "                'val_accuracy': trainer.val_acc_history[epoch]*100,\n",
    "                'val_loss': trainer.val_loss_history[epoch],\n",
    "                'epoch' : epoch\n",
    "            })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep_configuration, project=\"DA6401-Assignment-2\")\n",
    "\n",
    "    # Start sweep\n",
    "    wandb.agent('srtmx9ao', function=train_sweep, count=9)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7183288,
     "sourceId": 11463350,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7185063,
     "sourceId": 11465727,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 308982,
     "modelInstanceId": 288204,
     "sourceId": 344765,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
